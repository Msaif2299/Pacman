{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF ACTIONS: 16\n",
      "WARNING:tensorflow:From c:\\users\\msaif\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 874)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 874)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               112000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 179,984\n",
      "Trainable params: 179,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -19614.100, steps: 438\n",
      "Episode 2: reward: -38388.100, steps: 753\n",
      "Episode 3: reward: -17050.000, steps: 375\n",
      "Episode 4: reward: -23287.000, steps: 515\n",
      "Episode 5: reward: -41323.600, steps: 791\n",
      "Episode 6: reward: -26733.100, steps: 578\n",
      "Episode 7: reward: -22127.800, steps: 492\n",
      "Episode 8: reward: -18757.100, steps: 418\n",
      "Episode 9: reward: -34957.100, steps: 706\n",
      "Episode 10: reward: -23807.500, steps: 525\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append('../../keras-rl')\n",
    "import numpy as np\n",
    "import gym\n",
    "import _pac\n",
    "import pygame\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Convolution2D, Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "#We downsize the atari frame to 84 x 84 and feed the model 4 frames at a time for\n",
    "#a sense of direction and speed.\n",
    "INPUT_SHAPE = 34 + (30*28)\n",
    "WINDOW_LENGTH = 1\n",
    "'''\n",
    "#Standard Atari processing\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        print(observation.shape)\n",
    "        assert observation.ndim == 3\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--mode', choices=['train', 'test'], default='train')\n",
    "parser.add_argument('--env-name', type=str, default='pac-v0')\n",
    "parser.add_argument('--weights', type=str, default=None)\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "env_name = 'pac-v0'\n",
    "mode = 'test'\n",
    "weights = None\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(231)\n",
    "env.seed(123)\n",
    "nb_actions = 16\n",
    "print(\"NUMBER OF ACTIONS: \" + str(nb_actions))\n",
    "'''\n",
    "#Standard DQN model architecture.\n",
    "input_shape = (WINDOW_LENGTH, INPUT_SHAPE[0], INPUT_SHAPE[1])\n",
    "frame = Input(shape=(input_shape))\n",
    "cv1 = Convolution2D(32, kernel_size=(8,8), strides=4, activation='relu', data_format='channels_first')(frame)\n",
    "cv2 = Convolution2D(64, kernel_size=(4,4), strides=2, activation='relu', data_format='channels_first')(cv1)\n",
    "cv3 = Convolution2D(64, kernel_size=(3,3), strides=1, activation='relu', data_format='channels_first')(cv2)\n",
    "dense= Flatten()(cv3)\n",
    "dense = Dense(512, activation='relu')(dense)\n",
    "buttons = Dense(nb_actions, activation='linear')(dense)\n",
    "model = Model(inputs=frame,outputs=buttons)'''\n",
    "\n",
    "input_shape = (WINDOW_LENGTH, INPUT_SHAPE)\n",
    "frame = Input(shape=(input_shape))\n",
    "den1 = Flatten()(frame)\n",
    "den1 = Dense(128, activation='relu')(den1)\n",
    "den2 = Dense(256, activation='relu')(den1)\n",
    "den3 = Dense(128, activation='relu')(den2)\n",
    "buttons = Dense(nb_actions, activation='linear')(den3)\n",
    "model = Model(inputs=frame, outputs=buttons)\n",
    "print(model.summary())\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "#processor = AtariProcessor()\n",
    "\n",
    "policy =  LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1250000)\n",
    "\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
    "\n",
    "if mode == 'train':\n",
    "    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
    "    # can be prematurely aborted. Notice that now you can use the built-in Keras callbacks!\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "    checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
    "\n",
    "    # After training is done, we save the final weights one more time.\n",
    "    dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 10 episodes.\n",
    "    dqn.test(env, nb_episodes=10, visualize=False)\n",
    "elif mode == 'test':\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "    if weights:\n",
    "        weights_filename = weights\n",
    "    dqn.load_weights(weights_filename)\n",
    "    dqn.test(env, nb_episodes=10, visualize=True)\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
